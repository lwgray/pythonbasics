{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales_week4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_750 = sales.loc[0:750]\n",
    "group_by_Type = only_750.groupby('Type')\n",
    "agg = group_by_Type.agg({'Units':'mean', 'Sales':'sum'})\n",
    "agg.sort_values('Sales', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sales\n",
    ".loc[0:750]\n",
    ".groupby('Type')\n",
    ".agg({'Units':'mean', 'Sales':'sum'})\n",
    ".sort_values('Sales', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales.Units.isna().sum())\n",
    "print(sales.Units.fillna(200).isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date and Time\n",
    "Pandas uses special methods and data types to handle date and time.  Using these methods/data-types allows you to manipulate dates/time in unique ways.  You are able to subset and filter data based on date/time.  You are also able to aggregate data into time windows such as days, quarters, and years.\n",
    "* Epoch is January 1, 1970\n",
    "\n",
    "##  There are many abbreviations that represent date/time  frequency\n",
    "Documentation:  [DateOffsets](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases)\n",
    "\n",
    "<div align=left>\n",
    "\n",
    "    \n",
    "|letter|Object|\n",
    "|------|------|\n",
    "|M|Month|\n",
    "|D|Day|\n",
    "|Q|Quarterly|\n",
    "|H|Hourly|\n",
    "|W|Weekly|\n",
    "|Y|Yearly|\n",
    "|AS|YearStart|\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert column to datetime data type\n",
    "### Pandas `to_datetime` function\n",
    "This comes in useful for converting string columns in DataFrames to dates.\n",
    "* to_datetime can transform Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: use to_datetime\n",
    "crime = pd.read_csv('bmore_crime.csv')\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: use to_datetime\n",
    "crime['CrimeDateTime'] = pd.to_datetime(crime['CrimeDateTime'])\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert column by using the parse_dates parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2:   use parse_dates named parameter\n",
    "crime = pd.read_csv('bmore_crime.csv', parse_dates=['CrimeDateTime'])\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering columns with time data\n",
    "\n",
    "### Select all the rows in the CrimeDateTime column that have a certain date-time value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime[crime.CrimeDateTime == '2021-03-05 11:03:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial match with `.between()` method\n",
    "Allows you to do all the string searches above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime[crime.CrimeDateTime.between('2021-01-01', '2021-01-02')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To include both dates in `.between()` method  You must specify the end of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime[crime.CrimeDateTime.between('2021-3-4', '2021-3-5 23:59:59')].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dates with conditional expressions (VERY IMPORTANT!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_jan1_mar6 = crime[(crime.CrimeDateTime >= '2021-01-01 22') & (crime.CrimeDateTime <= '2021-03-06 11:22:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crime_jan1_mar6.groupby(['District', 'Description'])['Total_Incidents'].agg('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing time series intelligently\n",
    "We've covered dataframe selection and slicing before.\n",
    "We will introduce the concept of the DatetimeIndex.  We will examine\n",
    "the Baltimore crimes dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = crime.set_index('CrimeDateTime')\n",
    "print(crime.index[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatetimeIndex\n",
    "When we moved the CrimeDateTime column to the index with `set_index()` method we created a `DatetimeIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `.loc` to select all the rows equal to a single index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.loc['2021-03-05 23:22:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select all rows that partially match an index value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.loc['2021-03-05'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select all rows for a single month or year or hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.loc['2021-03'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crime.loc['2021'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.loc['2021-03-05 15'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contain the name of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.loc['Dec 2020'].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.loc['2014-03-06':'2021-03-05'].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using methods that only work with a DatetimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(crime.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `.between_time()` method\n",
    "Select all crimes that occurred between 2 A.M. and 5 A.M. regardless of the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.between_time('2:00', '5:00', include_end=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select all dates at a specific time with `.at_time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.at_time('5:47').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `first()` methods allow for selecting the first n segments of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.first('5D').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.first('3QS').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the number of weekly crimes\n",
    "We will use grouping according to some period of time to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.groupby(pd.Grouper(freq='W')).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating weekly homicides and arsons separately\n",
    "1. Group by each quarter then sum the IS_HOMICIDE and IS_ARSON columns for each group\n",
    "2. use resample method\n",
    "3. use groupby method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.groupby(pd.Grouper(freq='Q'))[['IS_HOMICIDE', 'IS_ARSON']].sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Quarterly Frequency of Homicides and Arson\n",
    "crime.groupby(pd.Grouper(freq='Q'))[['IS_HOMICIDE', 'IS_ARSON']].sum().plot(title='Baltimore Homicides and Arson', \n",
    "                                                                            color=['black', 'blue']);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by a Timestamp and another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee = pd.read_csv('employee.csv', parse_dates=['JOB_DATE', 'HIRE_DATE'], index_col='HIRE_DATE')\n",
    "employee.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first do a grouping by just gender, and find the average salary for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee.groupby('GENDER')['BASE_SALARY'].mean().round(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Let's find the average salary based on hire date, and group everyone into 10-year buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee.resample('10AS')['BASE_SALARY'].mean().round(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's group by gender and a 10-year time span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee.groupby('GENDER').resample('10AS')['BASE_SALARY'].mean().round(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Place both 10-year grouping and Gender within groupby\n",
    "employee.groupby(['GENDER', pd.Grouper(freq='10AS')])['BASE_SALARY'].mean().round(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair Programming\n",
    "1. Using `date_demo.csv` perform the following tasks\n",
    "* What is the shape of the data in the CSV file?\n",
    "* What is the min and max dates in the `birth_date` column?\n",
    "* How many people were born between 7/1/2019 & 7/1/2020? ie filter birth_date column and report number of rows\n",
    "\n",
    "2. Using `daily-min-temperatures.csv` which represents daily minimum temperature checks perform the following tasks\n",
    "* Convert `Date` column to datetime dtype\n",
    "* Create a new DataFrame that only contains data from 1982\n",
    "* Set index of new DataFrame to `Date` column\n",
    "* Group DataFrame by Weekly frequency then aggregate the `Temp` column by the aggregating function `mean`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales_week4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows that have nan values\n",
    "sales.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop only rows that have nan values in the Units column\n",
    "sales.dropna(subset=['Units'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns that contain nan values\n",
    "sales.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## duplicated() and drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.fillna(200).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[sales.Type.str.contains('Men')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[['Type', 'Department']] = sales.Type.str.split(expand=True)\n",
    "sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Type'] = sales.Type.str.upper()\n",
    "sales.Type.str.isupper().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code\n",
    "sales = pd.read_excel('https://github.com/datagy/pivot_table_pandas/raw/master/sample_pivot.xlsx',\n",
    "                      engine='openpyxl', parse_dates=['Date'])\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot with Region index and mean sales\n",
    "pivot_table1 = pd.pivot_table(sales, index='Region', values='Sales')\n",
    "pivot_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot with Region index and sum of sales\n",
    "pivot_table2 = pd.pivot_table(sales, index='Region', values='Sales', aggfunc='sum')\n",
    "pivot_table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot with Dual indices and sum of sales\n",
    "pivot_table3 = pd.pivot_table(sales, index=['Region', 'Type'], values='Sales', aggfunc='sum')\n",
    "pivot_table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot with columns and sum of sales\n",
    "pivot_table3 = pd.pivot_table(sales, index='Type', columns='Region', values='Sales', aggfunc='sum')\n",
    "pivot_table3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
